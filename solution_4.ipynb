{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAxwx-r3clcY"
   },
   "source": [
    "# RSE Problem 4 - Machine Translation\n",
    "\n",
    "The following is a simple end-to-end implementation of an LSTM encoder-decoder for translation from English to Spanish. I chose a sufficiently simple architecture that I was able to write a fully implemented solution that can be executed in this notebook, from data preparation, to the model definition, training/validation and evaluation. \n",
    "\n",
    "I've prepared a version that can be run in a Colab notebook [here](https://colab.research.google.com/drive/18eGiR2mBA69EOvQCC5uy3hzp3lo-ZaEx?usp=sharing) - I recommend using a GPU runtime (if available) otherwise the model training takes a good 20 mins (as opposed to about 3 mins with GPU)\n",
    "\n",
    "## Setup/Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W_ger4yeclcZ"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical, get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTuhZE_RZgmF"
   },
   "source": [
    "## Download Dataset\n",
    "\n",
    "We begin by loading the training dataset - I took the below code snippet from this example in the Keras documentation as a time saving exercise. (It also has the advantage of being english -> Spanish translations; I speak Spanish so it made debugging easier!)\n",
    "\n",
    "The end product is a list containing pairs of short English phrase strings and their translations in Spanish (examples in the output of the cell below). These are shuffled and then split into training/validation/test datasets to facilitate the rest of the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs02ZcnZclcb",
    "outputId": "a4af9f93-3d94-416f-97dd-00c8518dac21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Examples of text pairs:\n",
      "\n",
      "[\"I'm after him.\" 'Voy tras él.']\n",
      "['Close that door.' 'Cerrá esa puerta.']\n",
      "['That a boy!' '¡Ése es mi chico!']\n",
      "['Look at it.' 'Obsérvalo.']\n",
      "[\"It won't work.\" 'No servirá.']\n",
      "\n",
      "10000 total pairs\n",
      "7000 training pairs\n",
      "1500 validation pairs\n",
      "1500 test pairs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Download and read English -> Spanish dataset\n",
    "text_file = get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = None\n",
    "# Used only first 10,000 examples so model can be trained relatively quickly\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx == 10000:\n",
    "        break\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    if text_pairs is None:\n",
    "        text_pairs = np.array([[eng, spa]])\n",
    "    else:\n",
    "        text_pairs = np.append(text_pairs, [[eng, spa]], axis=0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Examples of text pairs:\\n\")\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "\n",
    "# Shuffle data and split into train/validation/test datasets\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train = text_pairs[:num_train_samples]\n",
    "val = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print()\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train)} training pairs\")\n",
    "print(f\"{len(val)} validation pairs\")\n",
    "print(f\"{len(test)} test pairs\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I71cGMMjZkJs"
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "As with most machine learning pipelines, a considerable amount of the complexity is in the data preparation phase. Machine translation models can't \"read\" raw text: they require text to be encoded or sequenced - converting raw text into a sequence of numbers. To do this, we fit a \"tokenizer\" for each language - tokenizers take the full vocabulary in the collection English/Spanish phrases and associate each word with a unique number. The text phrases are then represented by a vector of numbers equal length to the phrase, each number representing the corresponding word. The labels or ground truths are then \"one-hot encoded\" - this converts the sequenced vectors (length = maximum phrase length) into a 2-dimensional matrix (vocabulary size x maximum phrase length) with a 1 in the row position of the value of the correctly translated word and a zero in the other positions. \n",
    "\n",
    "Examples of outputs from each stage of the encoding process can be seen in the output below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi-VG_mXHTsl",
    "outputId": "499c21f1-6593-4562-e203-1228f80487d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence:\n",
      "\n",
      "Ve.\n",
      "\n",
      "... sequenced:\n",
      "\n",
      "[50  0  0  0  0  0  0  0]\n",
      "\n",
      "... and one-hot encoded:\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# function to fit tokenizer for each language\n",
    "def return_fitted_tokenizer(texts):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(texts)\n",
    "\treturn tokenizer\n",
    " \n",
    "def calculate_max_length(texts):\n",
    "\treturn max(len(text.split()) for text in texts)\n",
    " \n",
    "def sequence_texts(tokenizer, maxlen, texts):\n",
    "\tsequences = tokenizer.texts_to_sequences(texts)\n",
    "\tsequences = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\treturn sequences \n",
    " \n",
    "### Prep English examples\n",
    "\n",
    "# fit tokeniser and calculate vocab size and max sentence length\n",
    "eng_tokenizer = return_fitted_tokenizer(text_pairs[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = calculate_max_length(text_pairs[:, 0])\n",
    "\n",
    "# Encode English train/validation/test data\n",
    "X_train = sequence_texts(eng_tokenizer, eng_length, train[:, 0])\n",
    "X_val = sequence_texts(eng_tokenizer, eng_length, val[:, 0])\n",
    "X_test = sequence_texts(eng_tokenizer, eng_length, test[:, 0])\n",
    "\n",
    "### Prep Spanish examples\n",
    "# same process as english examples, but sequenced examples are one-hot encoded\n",
    "\n",
    "spa_tokenizer = return_fitted_tokenizer(text_pairs[:, 1])\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "spa_length = calculate_max_length(text_pairs[:, 1])\n",
    "\n",
    "print(\"Example sentence:\\n\")\n",
    "print(train[0,1])\n",
    "\n",
    "Y_train = sequence_texts(spa_tokenizer, spa_length, train[:, 1])\n",
    "\n",
    "print(\"\\n... sequenced:\\n\")\n",
    "print(Y_train[0])\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes=spa_vocab_size)\n",
    "\n",
    "print(\"\\n... and one-hot encoded:\\n\")\n",
    "print(Y_train[0])\n",
    "\n",
    "Y_val = sequence_texts(spa_tokenizer, spa_length, val[:, 1])\n",
    "Y_val = to_categorical(Y_val, num_classes=spa_vocab_size)\n",
    "\n",
    "Y_test = sequence_texts(spa_tokenizer, spa_length, test[:, 1])\n",
    "Y_test = to_categorical(Y_test, num_classes=spa_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkV7fPI1Znnq"
   },
   "source": [
    "## Build model\n",
    "\n",
    "I decided on a simple LSTM encoder/decoder framework, as it is an effective and well established technique for machine translation, and can be easily implemented in a short timeframe. I chose to implement the model using the Keras package, and specifically their \"sequential\" framework, that allows models to be built as a simple stack of pre-implemented layers with a single input and output. I defined the model with sufficient complexity that it can achieve reasonable performance on the given task, but not with so many free-parameters that it would take hours and hours to train. The simplicity of this implementation can be seen below - the code implements an initial embedding layer, an LSTM encoder and Decoder, and a fully connected output layer, all in a few short lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6t6qbq0F8dv",
    "outputId": "912f4fc8-bfe8-4e6f-a81e-0322395458f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 5, 256)            404224    \n",
      "_________________________________________________________________\n",
      "encoder (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "decoder (LSTM)               (None, 8, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 8, 3182)           817774    \n",
      "=================================================================\n",
      "Total params: 2,272,622\n",
      "Trainable params: 2,272,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define and compile Keras LSTM model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(eng_vocab_size, \n",
    "              256, \n",
    "              input_length=eng_length, \n",
    "              mask_zero=True, \n",
    "              name = \"embedding_layer\")\n",
    ")\n",
    "model.add(LSTM(256, name=\"encoder\"))\n",
    "model.add(RepeatVector(spa_length))\n",
    "model.add(LSTM(256, return_sequences=True, name=\"decoder\"))\n",
    "model.add(TimeDistributed(\n",
    "        Dense(spa_vocab_size, \n",
    "              activation='softmax', \n",
    "              name = \"fully_connected_layer\")\n",
    "))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PfKr9rrZtyH"
   },
   "source": [
    "## LSTM\n",
    "\n",
    "An LSTM is a specific implementation of recurrent neural network or RNN. RNNs can take a variable length sequence, and encode that sequence into a vector representation one time step at a time. The \"recurrent\" naming of an RNN signifies that the network uses a recurring set of neurons and weights to encode each timestep in the sequence. Each recurring unit passes a \"state\" signal from one time step to the next - this signal is combined with the input at each timestep to calculate a new \"state\" signal to be passed along to the next timestep. \n",
    "\n",
    "In the case of language translation, RNNs are structured in an \"encoder\" / \"decoder\" arrangement. The encoder takes the input text sequence and encodes it into a vector representation of the text, as described above. This vector representation isn't human readable/understandable, so a separate decoder network is used to translate the sentence into the target language. The decoder network uses the same recurrent process as the encoder, but takes the encoded signal as an input, and at each timestep outputs a set of probabilities that signify the likelihood that a word in the target language appears at that timestep in the sequence.\n",
    "\n",
    "RNNs suffer significantly exploding/vanishing gradients. Put simply, the longer the encoded/decoded sequence is, the more often the signal propagating through the network is multiplied by the recurrent network weights - the result is an exploding signal (when the weights are > 1) or vanishing signal (when the weights are < 1). LSTMs overcome this issue by allowing the network to \"forget\" certain aspects of the signal propagating from one timestep to the next. The architecture includes \"gated\" units, with sets of free parameters that allow each recurrent unit to determine:\n",
    " * the parts of the input signal to update\n",
    " * the size of the updates that should be made\n",
    " * what parts of the signal to output to the next timestep\n",
    "\n",
    "By allowing parts of the signal to propagate through the network unchanged, LSTMs largely avoid the issue of exploding/vanishing gradients, and can be used to encode/decode much longer signals than more simple recurrent architectures. \n",
    "\n",
    "## Model Training / Validation\n",
    "\n",
    "The following code fragment implements the model training loop. As with many of the python machine learning frameworks, Keras models are trained using the `model.fit()` method of the instantiated model, with a few simple hyperparameters to determine the details of the training loop. At the end of each training epoch, the performance of the model is evaluated against the validation dataset - in a more complex pipeline, this could be used as a benchmark to choose between different model architectures (e.g. numbers of layers, dropout rates, optimisation functions) and hyperparameters (e.g. learning rates, number of training iterations, early stopping), and avoids biasing the final model towards performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3PQnJWoZsQZ",
    "outputId": "5a1174b6-bf1e-4120-b8ff-e6e9b2582646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/110 - 12s - loss: 3.2057 - val_loss: 2.7223\n",
      "Epoch 2/30\n",
      "110/110 - 2s - loss: 2.1850 - val_loss: 2.5924\n",
      "Epoch 3/30\n",
      "110/110 - 2s - loss: 2.0513 - val_loss: 2.5094\n",
      "Epoch 4/30\n",
      "110/110 - 2s - loss: 1.9583 - val_loss: 2.4193\n",
      "Epoch 5/30\n",
      "110/110 - 3s - loss: 1.8769 - val_loss: 2.3843\n",
      "Epoch 6/30\n",
      "110/110 - 2s - loss: 1.7928 - val_loss: 2.3482\n",
      "Epoch 7/30\n",
      "110/110 - 2s - loss: 1.7117 - val_loss: 2.3002\n",
      "Epoch 8/30\n",
      "110/110 - 2s - loss: 1.6237 - val_loss: 2.2462\n",
      "Epoch 9/30\n",
      "110/110 - 2s - loss: 1.5310 - val_loss: 2.2046\n",
      "Epoch 10/30\n",
      "110/110 - 2s - loss: 1.4336 - val_loss: 2.1550\n",
      "Epoch 11/30\n",
      "110/110 - 2s - loss: 1.3358 - val_loss: 2.0933\n",
      "Epoch 12/30\n",
      "110/110 - 2s - loss: 1.2416 - val_loss: 2.0498\n",
      "Epoch 13/30\n",
      "110/110 - 2s - loss: 1.1473 - val_loss: 1.9934\n",
      "Epoch 14/30\n",
      "110/110 - 2s - loss: 1.0664 - val_loss: 1.9505\n",
      "Epoch 15/30\n",
      "110/110 - 2s - loss: 0.9813 - val_loss: 1.9009\n",
      "Epoch 16/30\n",
      "110/110 - 2s - loss: 0.9054 - val_loss: 1.8750\n",
      "Epoch 17/30\n",
      "110/110 - 2s - loss: 0.8392 - val_loss: 1.8383\n",
      "Epoch 18/30\n",
      "110/110 - 2s - loss: 0.7788 - val_loss: 1.8129\n",
      "Epoch 19/30\n",
      "110/110 - 2s - loss: 0.7237 - val_loss: 1.7883\n",
      "Epoch 20/30\n",
      "110/110 - 2s - loss: 0.6709 - val_loss: 1.7758\n",
      "Epoch 21/30\n",
      "110/110 - 2s - loss: 0.6259 - val_loss: 1.7414\n",
      "Epoch 22/30\n",
      "110/110 - 2s - loss: 0.5853 - val_loss: 1.7350\n",
      "Epoch 23/30\n",
      "110/110 - 2s - loss: 0.5474 - val_loss: 1.7153\n",
      "Epoch 24/30\n",
      "110/110 - 2s - loss: 0.5127 - val_loss: 1.7019\n",
      "Epoch 25/30\n",
      "110/110 - 2s - loss: 0.4818 - val_loss: 1.6826\n",
      "Epoch 26/30\n",
      "110/110 - 2s - loss: 0.4518 - val_loss: 1.6701\n",
      "Epoch 27/30\n",
      "110/110 - 2s - loss: 0.4270 - val_loss: 1.6593\n",
      "Epoch 28/30\n",
      "110/110 - 2s - loss: 0.4027 - val_loss: 1.6451\n",
      "Epoch 29/30\n",
      "110/110 - 3s - loss: 0.3801 - val_loss: 1.6380\n",
      "Epoch 30/30\n",
      "110/110 - 2s - loss: 0.3605 - val_loss: 1.6304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabc0169fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model checkpoint to save model at each epoch\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=0,\n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# Initiate training loop - 30 \"epochs\" or iterations\n",
    "model.fit(X_train, Y_train, \n",
    "          epochs=30, \n",
    "          batch_size=64, \n",
    "          validation_data=(X_val, Y_val), \n",
    "          callbacks=[checkpoint], \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "easHBJEIZvBi"
   },
   "source": [
    "Below are some examples of output from trained model. It performs relatively well given the short training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUjeYK9sP2bK",
    "outputId": "11317ef4-cc99-4d81-8e19-5c0d5cb528f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's his => es le de. Truth: Es suyo.\n",
      "answer tom => ¡respóndale a tomás. Truth: ¡Respóndanle a Tomás!\n",
      "tom vanished => tom se ahogó. Truth: Tomás desapareció.\n",
      "it's no joke => es un pez. Truth: No es una broma.\n",
      "you go first => tú primero. Truth: Vosotros primero.\n",
      "that's for you => no es es. Truth: Es para ti.\n",
      "you were lucky => traed centrarte. Truth: La sacaste barata.\n",
      "what a pity => ¡qué lástima. Truth: Qué lástima.\n",
      "i need it asap => yo tengo lo. Truth: Lo necesito tan pronto como sea posible.\n",
      "that's so lame => eso es un patético. Truth: Eso es tan patético.\n",
      "i didn't walk => yo no voté. Truth: Yo no caminé.\n",
      "i write poems => me siento mal. Truth: Escribo versos.\n",
      "tom drove fast => ella fue. Truth: Tom condujo rápido.\n",
      "i'm fair => estoy estoy. Truth: Soy justo.\n",
      "stop singing => deja de. Truth: Deja de cantar.\n",
      "come at once => ven enseguida. Truth: Ven enseguida.\n",
      "quiet please => por favor silencio. Truth: Por favor, silencio.\n",
      "we love it => lo encanta. Truth: Nos encanta.\n",
      "we can meet => podemos vernos. Truth: Podemos encontrarnos.\n",
      "i'm unlucky => estoy mejor. Truth: Tengo mala suerte.\n"
     ]
    }
   ],
   "source": [
    "# Generate examples of translations from test data\n",
    "def generate_eg_predictions():\n",
    "    random_idxs = np.random.choice(100, 20)\n",
    "    eng_sequences = X_test[random_idxs,:]\n",
    "    preds = model.predict(eng_sequences)\n",
    "    pred_sequences = np.argmax(preds, axis=2)\n",
    "    eng_sentences = eng_tokenizer.sequences_to_texts(eng_sequences)\n",
    "    pred_sentences = spa_tokenizer.sequences_to_texts(pred_sequences)\n",
    "    ground_truthes = test[random_idxs, 1]\n",
    "    sentences = zip(eng_sentences, pred_sentences, ground_truthes)\n",
    "    for eng_sentence, pred_sentence, truth in sentences:\n",
    "        print(f\"{eng_sentence} => {pred_sentence}. Truth: {truth}\")\n",
    "\n",
    "generate_eg_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZyKE7NkZwb7"
   },
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "Once model selection and parameter tuning are complete, the final model(s) can be evaluated agains the holdout test set. This is data that hasn't been used in model training/validation and so is a (relatively) unbiased estimator of model performance on unseen data. A common scoring methodology in machine translation is the BLEU (Bilingual Evaluation Understudy) - a score from 0 to 1 that evaluates the similarity of model translations to the ground truth translations. \n",
    "\n",
    "The following code uses the NLTK implementation of the BLEU score to evaluate the model we've trained:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LO_vU2UEmgpI",
    "outputId": "e7bd0bed-7807-4dd0-e540-3de6711e351b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test Bleu score is 0.8566313698811789\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance against Bleu score\n",
    "preds = model.predict(X_test)\n",
    "pred_sequences = np.argmax(preds, axis=2)\n",
    "pred_sentences = spa_tokenizer.sequences_to_texts(pred_sequences)\n",
    "ground_truthes = test[:, 1]\n",
    "bleu_score = corpus_bleu(ground_truthes, pred_sentences)\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Bleu score is {bleu_score}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trBywM6lCwfs"
   },
   "source": [
    "Note: This implementation of the BLEU score is not well suited to translations of the length used in this example - hence the warning message and the overinflated score (the model doesn't perform nearly well enough to achieve above 0.7). Unfortunately I didn't have the time to explore the alternatives available in the NLTK SmoothingFunction() implementation of BLEU, but I would certainly do so with more time. I thought it best to leave in as an example of how one might evaluate the performance of a model like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O6HFvBnWYF6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RSE Problem 4 - Machine Translation",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
